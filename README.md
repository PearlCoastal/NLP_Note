#  Natural Language Processing
Basic acknowledge of natural language processing models.

1. Attention Mechanism: Reasons why attention.
2. Kinds of attention: soft & hard attention mechanism.
3. Difference between attention and self-attention.
4. How self-attention worked.
5. About multi-head self-attention.
6. Some classifier function: Sigmoid, Softmax, Tanh and ReLu.
7. Little about CNN and RNN.
8. Vanishing gradient problem & explosion gradient problem.
9. How LSTM solved long-term dependencies problem.
10. Difference between LSTM & GRU.
11. How transformer solved long-distance dependencies problem.
12. Descrption of transformer models: Transformer Encoder & Transformer Decoder.
13. Difference between Transformer Encoder & Transformer Decoder.

ðŸ‘‰ [Note](https://github.com/PearlCoastal/NLP_Note/blob/master/AttentionMechanism.md)

# Machine Learning Note

1. About logistic regression & linear regression.
2. Some supervised learning methods: KNN, SVM, Kernel-SVM, Decision Tree and Naive Bayes.
3. Ensemble Learningï¼š Bagging & Boosting.
4. Comparison of Bagging & Boosting.
5. Difference between logistic regression & linear regression.

ðŸ‘‰[Note](https://github.com/PearlCoastal/NLP_Note/blob/master/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.md)
